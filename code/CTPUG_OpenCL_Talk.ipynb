{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Program Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting platforms &/ devices\n",
    "Looking at what is available, then selecting the NVIDIA one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) OpenCL\n",
      "Portable Computing Language\n",
      "NVIDIA CUDA\n"
     ]
    }
   ],
   "source": [
    "ocl_platforms = (platform.name \n",
    "                 for platform in pyopencl.get_platforms())\n",
    "print(\"\\n\".join(ocl_platforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_platform = [platform \n",
    "                   for platform in pyopencl.get_platforms() \n",
    "                   if platform.name == \"NVIDIA CUDA\"][0]\n",
    "nvidia_devices = nvidia_platform.get_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Programs\n",
    "Using the PyOpenCL bindings to create an OpenCL context, then declaring OpenCL kernel code, and compiling it.\n",
    "\n",
    "The code is for a simple vector sum, i.e. $\\vec{c} = \\vec{a} + \\vec{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_context = pyopencl.Context(devices=nvidia_devices)\n",
    "\n",
    "program_source = \"\"\"\n",
    "      kernel void sum(global float *a, \n",
    "                      global float *b,\n",
    "                      global float *c){\n",
    "        int gid = get_global_id(0);\n",
    "        c[gid] = a[gid] + b[gid];\n",
    "      }\n",
    "    \"\"\"\n",
    "nvidia_program_source = pyopencl.Program(nvidia_context, program_source)\n",
    "nvidia_program = nvidia_program_source.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Names: sum\n"
     ]
    }
   ],
   "source": [
    "program_kernel_names = nvidia_program.get_info(pyopencl.program_info.KERNEL_NAMES)\n",
    "print(\"Kernel Names: {}\".format(program_kernel_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Programs\n",
    "Actually running the code using the Python bindings. \n",
    "\n",
    "Some memory and data needs to be setup first, but I'll elaborate on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocl_kernel(queue, \n",
    "                   kernel, \n",
    "                   global_size,\n",
    "                   input_tuples,\n",
    "                   output_tuples,\n",
    "                   local_size = (32,)):\n",
    "    \n",
    "    # copying data onto the device\n",
    "    for (array, buffer) in input_tuples:\n",
    "        pyopencl.enqueue_copy(queue, src=array, dest=buffer)\n",
    "    \n",
    "    # running program on the device\n",
    "    kernel_arguments  = [buffer for (arr,buffer) in input_tuples] \n",
    "    kernel_arguments += [buffer for (arr,buffer) in output_tuples]\n",
    "        \n",
    "    kernel(queue,\n",
    "           global_size,\n",
    "           local_size,\n",
    "           *kernel_arguments)\n",
    "\n",
    "    # copying data off the device\n",
    "    for (arr, buffer) in output_tuples:\n",
    "        pyopencl.enqueue_copy(queue, src=buffer, dest=arr)\n",
    "        \n",
    "    # waiting for everything to finish\n",
    "    queue.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sum_results(a,b,c):\n",
    "    c_ref = a + b\n",
    "    err = numpy.abs(c - c_ref)\n",
    "    if((err.sum() > 0.0).any()): \n",
    "        print(\"result does not match\")\n",
    "    else: \n",
    "        print(\"result matches!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data setup\n",
    "N = int(2**20)\n",
    "a = numpy.random.rand(N).astype(numpy.float32)\n",
    "b = numpy.random.rand(N).astype(numpy.float32)\n",
    "c = numpy.empty_like(a)\n",
    "\n",
    "# Device Memory setup\n",
    "a_nvidia_buffer = pyopencl.Buffer(nvidia_context,\n",
    "                                  flags=pyopencl.mem_flags.READ_ONLY, \n",
    "                                  size=a.nbytes)\n",
    "b_nvidia_buffer = pyopencl.Buffer(nvidia_context, \n",
    "                                  flags=pyopencl.mem_flags.READ_ONLY, \n",
    "                                  size=b.nbytes)\n",
    "c_nvidia_buffer = pyopencl.Buffer(nvidia_context, \n",
    "                                  flags=pyopencl.mem_flags.WRITE_ONLY, \n",
    "                                  size=c.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_queue = pyopencl.CommandQueue(nvidia_context)\n",
    "\n",
    "input_tuples = ((a, a_nvidia_buffer), (b, b_nvidia_buffer), )\n",
    "output_tuples = ((c, c_nvidia_buffer),)\n",
    "run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result matches!\n"
     ]
    }
   ],
   "source": [
    "check_sum_results(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14 ms ± 117 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Suggestion***: Characterise the OpenCL devices you can get access to, stepping up the size of $N$ by some factor (power of 2 is easy to reason about). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Manipulate Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using global memory\n",
    "Differentiating between different memory flags, and defining some utility functions to help set up memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_memory(context, input_arrays):\n",
    "    return [\n",
    "        (array, pyopencl.Buffer(context,\n",
    "                                flags=pyopencl.mem_flags.READ_ONLY, \n",
    "                                size=array.nbytes))\n",
    "        for array in input_arrays\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_memory(context, output_arrays):\n",
    "    return [\n",
    "        (array, pyopencl.Buffer(context,\n",
    "                                flags=pyopencl.mem_flags.WRITE_ONLY, \n",
    "                                size=array.nbytes))\n",
    "        for array in output_arrays\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memories(tuples):\n",
    "    for (_, buffer) in tuples:\n",
    "        buffer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result matches!\n"
     ]
    }
   ],
   "source": [
    "a = numpy.random.rand(N).astype(numpy.float32)\n",
    "b = numpy.random.rand(N).astype(numpy.float32)\n",
    "c = numpy.empty_like(a)\n",
    "\n",
    "# Device Memory setup\n",
    "input_tuples = create_input_memory(nvidia_context, (a,b,))\n",
    "output_tuples = create_output_memory(nvidia_context, (c,),)\n",
    "run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples)\n",
    "check_sum_results(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using local and private memory\n",
    "Exploring the use of more localised memories to optimise performance. \n",
    "\n",
    "Most of the difference lives in the kernels, which are defined in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_program(ocl_file, context, build_options=[]):\n",
    "    with open(ocl_file,\"r\") as opencl_file:\n",
    "        program_source_code = opencl_file.read()\n",
    "        program_source = pyopencl.Program(context, program_source_code)\n",
    "        program = program_source.build(options=build_options)\n",
    "        \n",
    "        return program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result matches!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "wg_size = 32\n",
    "power = 1\n",
    "build_options = [\"-D\",\"BATCH_SIZE={}\".format(batch_size),\n",
    "                 \"-D\",\"WG_SIZE={}\".format(wg_size),\n",
    "                 \"-D\",\"POW={}\".format(power)]\n",
    "\n",
    "nvidia_program = build_program(\"vector_sum.cl\", nvidia_context, build_options)\n",
    "run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples, (wg_size,))\n",
    "check_sum_results(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.97 ms ± 121 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum_batched, (N//batch_size,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.02 ms ± 22.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum16, (N//16,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 ms ± 37.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum_batched_private, (N//batch_size,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ms ± 23.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum16_local, (N//16,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 µs ± 18.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = 1000\n",
    "build_options = [\"-D\",\"BATCH_SIZE={}\".format(batch_size),\n",
    "                 \"-D\",\"WG_SIZE={}\".format(wg_size),\n",
    "                 \"-D\",\"POW={}\".format(power)]\n",
    "\n",
    "nvidia_program = build_program(\"vector_sum.cl\", nvidia_context, build_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.74 ms ± 421 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum16_pow, (N//16,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gordon/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in power\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.7 ms ± 1.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (a+b)**power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Do Things in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device characteristics\n",
    "Using the host code API to inspect the characteristics of the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "intel_platform = [platform \n",
    "                  for platform in pyopencl.get_platforms() \n",
    "                  if platform.name == \"Portable Computing Language\"][0]\n",
    "intel_devices = [intel_platform.get_devices()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_properties = {\n",
    "    \"Device Name\":pyopencl.device_info.NAME,\n",
    "    \"Device Platform\":pyopencl.device_info.PLATFORM,\n",
    "    \"Device Type\":pyopencl.device_info.TYPE\n",
    "}\n",
    "\n",
    "processing_properties = {\n",
    "    \"Available Compute Units\": pyopencl.device_info.MAX_COMPUTE_UNITS,\n",
    "    \"Clockrate\": pyopencl.device_info.MAX_CLOCK_FREQUENCY\n",
    "}\n",
    "\n",
    "memory_properties = {\n",
    "    \"Available Global Memory\": pyopencl.device_info.GLOBAL_MEM_SIZE,\n",
    "    \"Available Constant Memory\": pyopencl.device_info.MAX_CONSTANT_BUFFER_SIZE,\n",
    "    \"Available Local Memory\" : pyopencl.device_info.LOCAL_MEM_SIZE\n",
    "}\n",
    "\n",
    "device_types = {\n",
    "    pyopencl.device_type.CPU:\"CPU\",\n",
    "    pyopencl.device_type.GPU:\"GPU\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: pthread-Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "Device Platform: <pyopencl.Platform 'Portable Computing Language' at 0x7fa60919e020>\n",
      "Device Types: CPU\n",
      "Available Compute Units: 8\n",
      "Clockrate: 3800\n",
      "Available Constant Memory: 4194304\n",
      "Available Global Memory: 14515052544\n",
      "Available Local Memory: 4194304\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for device in (intel_devices):\n",
    "    #print out all of the device name properties, except the device type\n",
    "    for property_name in sorted(name_properties.keys() - {\"Device Type\"}):\n",
    "        property_string_args = (property_name,device.get_info(name_properties[property_name]))\n",
    "        print(\"{}: {}\".format(*property_string_args))\n",
    "        \n",
    "    #look up the device type\n",
    "    print(\"Device Types: {}\".format(device_types[device.get_info(name_properties[\"Device Type\"])]))\n",
    "    \n",
    "    #print out all of the processing properties\n",
    "    for property_name in sorted(processing_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(processing_properties[property_name]))\n",
    "        print(\"{}: {}\".format(*property_string_args))\n",
    "    \n",
    "    #print out all of the memory properties\n",
    "    for property_name in sorted(memory_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(memory_properties[property_name]))\n",
    "        print(\"{}: {}\".format(*property_string_args))\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of platforms                               3\n",
      "  Platform Name                                   Intel(R) OpenCL\n",
      "  Platform Vendor                                 Intel(R) Corporation\n",
      "  Platform Version                                OpenCL 2.0 \n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_depth_images cl_khr_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_icd cl_khr_image2d_from_buffer cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_spir\n",
      "  Platform Extensions function suffix             INTEL\n",
      "\n",
      "  Platform Name                                   Portable Computing Language\n",
      "  Platform Vendor                                 The pocl project\n",
      "  Platform Version                                OpenCL 1.2 pocl 1.1 None+Asserts, LLVM 6.0.0, SPIR, SLEEF, DISTRO, POCL_DEBUG\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_icd\n",
      "  Platform Extensions function suffix             POCL\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "  Platform Vendor                                 NVIDIA Corporation\n",
      "  Platform Version                                OpenCL 1.2 CUDA 9.1.84\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_nv_create_buffer\n",
      "  Platform Extensions function suffix             NV\n",
      "\n",
      "  Platform Name                                   Intel(R) OpenCL\n",
      "Number of devices                                 2\n",
      "  Device Name                                     Intel(R) HD Graphics\n",
      "  Device Vendor                                   Intel(R) Corporation\n",
      "  Device Vendor ID                                0x8086\n",
      "  Device Version                                  OpenCL 2.0 \n",
      "  Driver Version                                  r5.0.63503\n",
      "  Device OpenCL C Version                         OpenCL C 2.0 \n",
      "  Device Type                                     GPU\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               23\n",
      "  Max clock frequency                             1100MHz\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     0\n",
      "    Supported partition types                     by <unknown> (0x559800000000)\n",
      "    Supported affinity domains                    0x559800000000\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             256x256x256\n",
      "  Max work group size                             256\n",
      "  Preferred work group size multiple              32\n",
      "  Sub-group sizes (Intel)                         8, 16, 32\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                16 / 16      \n",
      "    short                                                8 / 8       \n",
      "    int                                                  4 / 4       \n",
      "    long                                                 1 / 1       \n",
      "    half                                                 8 / 8        (cl_khr_fp16)\n",
      "    float                                                1 / 1       \n",
      "    double                                               1 / 1        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (cl_khr_fp16)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  Global memory size                              13321640346 (12.41GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           4294959103 (4GiB)\n",
      "  Unified memory for Host and Device              Yes\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       1024 bits (128 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           64 bytes\n",
      "    Global                                        64 bytes\n",
      "    Local                                         64 bytes\n",
      "  Max size for global variable                    65536 (64KiB)\n",
      "  Preferred total size of global vars             4294959103 (4GiB)\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        524288 (512KiB)\n",
      "  Global Memory cache line size                   64 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             16\n",
      "    Max size for 1D images from buffer            268434943 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   4 bytes\n",
      "    Pitch alignment for 2D image buffers          4 pixels\n",
      "    Max 2D image size                             16384x16384 pixels\n",
      "    Max planar YUV image size                     16384x16380 pixels\n",
      "    Max 3D image size                             16384x16384x2048 pixels\n",
      "    Max number of read image args                 128\n",
      "    Max number of write image args                128\n",
      "    Max number of read/write image args           128\n",
      "  Max number of pipe args                         16\n",
      "  Max active pipe reservations                    1\n",
      "  Max pipe packet size                            1024\n",
      "  Local memory type                               Local\n",
      "  Local memory size                               65536 (64KiB)\n",
      "  Max number of constant args                     8\n",
      "  Max constant buffer size                        4294959103 (4GiB)\n",
      "  Max size of kernel argument                     1024\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "    Preferred size                                131072 (128KiB)\n",
      "    Max size                                      67108864 (64MiB)\n",
      "  Max queues on device                            1\n",
      "  Max events on device                            1024\n",
      "  Prefer user sync for interop                    Yes\n",
      "  Profiling timer resolution                      83ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            No\n",
      "    SPIR versions                                 1.2 \n",
      "  printf() buffer size                            4194304 (4MiB)\n",
      "  Built-in kernels                                block_motion_estimate_intel;block_advanced_motion_estimate_check_intel;block_advanced_motion_estimate_bidirectional_check_intel\n",
      "  Motion Estimation accelerator version (Intel)   2\n",
      "    Device-side AVC Motion Estimation version     1\n",
      "      Supports texture sampler use                Yes\n",
      "      Supports preemption                         No\n",
      "  Device Extensions                               cl_intel_accelerator cl_intel_advanced_motion_estimation cl_intel_device_side_avc_motion_estimation cl_intel_driver_diagnostics cl_intel_media_block_io cl_intel_motion_estimation cl_intel_planar_yuv cl_intel_packed_yuv cl_intel_required_subgroup_size cl_intel_subgroups cl_intel_subgroups_short cl_intel_va_api_media_sharing cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_depth_images cl_khr_fp16 cl_khr_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_icd cl_khr_image2d_from_buffer cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_mipmap_image cl_khr_mipmap_image_writes cl_khr_spir cl_khr_subgroups \n",
      "\n",
      "  Device Name                                     Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "  Device Vendor                                   Intel(R) Corporation\n",
      "  Device Vendor ID                                0x8086\n",
      "  Device Version                                  OpenCL 2.0 (Build 475)\n",
      "  Driver Version                                  1.2.0.475\n",
      "  Device OpenCL C Version                         OpenCL C 2.0 \n",
      "  Device Type                                     CPU\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               8\n",
      "  Max clock frequency                             2800MHz\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     8\n",
      "    Supported partition types                     by counts, equally, by names (Intel)\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             8192x8192x8192\n",
      "  Max work group size                             8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preferred work group size multiple              128\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                 1 / 32      \n",
      "    short                                                1 / 16      \n",
      "    int                                                  1 / 8       \n",
      "    long                                                 1 / 4       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                1 / 8       \n",
      "    double                                               1 / 4        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 No\n",
      "    Round to infinity                             No\n",
      "    IEEE754-2008 fused multiply-add               No\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  No\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  Global memory size                              16662536192 (15.52GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           4165634048 (3.88GiB)\n",
      "  Unified memory for Host and Device              Yes\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       1024 bits (128 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           64 bytes\n",
      "    Global                                        64 bytes\n",
      "    Local                                         0 bytes\n",
      "  Max size for global variable                    65536 (64KiB)\n",
      "  Preferred total size of global vars             65536 (64KiB)\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        262144 (256KiB)\n",
      "  Global Memory cache line size                   64 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             480\n",
      "    Max size for 1D images from buffer            260352128 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   64 bytes\n",
      "    Pitch alignment for 2D image buffers          64 pixels\n",
      "    Max 2D image size                             16384x16384 pixels\n",
      "    Max 3D image size                             2048x2048x2048 pixels\n",
      "    Max number of read image args                 480\n",
      "    Max number of write image args                480\n",
      "    Max number of read/write image args           480\n",
      "  Max number of pipe args                         16\n",
      "  Max active pipe reservations                    32767\n",
      "  Max pipe packet size                            1024\n",
      "  Local memory type                               Global\n",
      "  Local memory size                               32768 (32KiB)\n",
      "  Max number of constant args                     480\n",
      "  Max constant buffer size                        131072 (128KiB)\n",
      "  Max size of kernel argument                     3840 (3.75KiB)\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "    Local thread execution (Intel)                Yes\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "    Preferred size                                4294967295 (4GiB)\n",
      "    Max size                                      4294967295 (4GiB)\n",
      "  Max queues on device                            4294967295\n",
      "  Max events on device                            4294967295\n",
      "  Prefer user sync for interop                    No\n",
      "  Profiling timer resolution                      1ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            Yes\n",
      "    SPIR versions                                 1.2\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                \n",
      "  Device Extensions                               cl_khr_icd cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_depth_images cl_khr_3d_image_writes cl_intel_exec_by_local_thread cl_khr_spir cl_khr_fp64 cl_khr_image2d_from_buffer \n",
      "\n",
      "  Platform Name                                   Portable Computing Language\n",
      "Number of devices                                 1\n",
      "  Device Name                                     pthread-Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "  Device Vendor                                   GenuineIntel\n",
      "  Device Vendor ID                                0x8086\n",
      "  Device Version                                  OpenCL 1.2 pocl HSTR: pthread-x86_64-pc-linux-gnu-skylake\n",
      "  Driver Version                                  1.1\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 pocl\n",
      "  Device Type                                     CPU\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               8\n",
      "  Max clock frequency                             3800MHz\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     8\n",
      "    Supported partition types                     equally, by counts\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             4096x4096x4096\n",
      "  Max work group size                             4096\n",
      "  Preferred work group size multiple              8\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                16 / 16      \n",
      "    short                                               16 / 16      \n",
      "    int                                                  8 / 8       \n",
      "    long                                                 4 / 4       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                8 / 8       \n",
      "    double                                               4 / 4        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  Global memory size                              14515052544 (13.52GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           4294967296 (4GiB)\n",
      "  Unified memory for Host and Device              Yes\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       1024 bits (128 bytes)\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        6291456 (6MiB)\n",
      "  Global Memory cache line size                   64 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             16\n",
      "    Max size for 1D images from buffer            268435456 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Max 2D image size                             16384x16384 pixels\n",
      "    Max 3D image size                             2048x2048x2048 pixels\n",
      "    Max number of read image args                 128\n",
      "    Max number of write image args                128\n",
      "  Local memory type                               Global\n",
      "  Local memory size                               4194304 (4MiB)\n",
      "  Max number of constant args                     8\n",
      "  Max constant buffer size                        4194304 (4MiB)\n",
      "  Max size of kernel argument                     1024\n",
      "  Queue properties                                \n",
      "    Out-of-order execution                        No\n",
      "    Profiling                                     Yes\n",
      "  Prefer user sync for interop                    Yes\n",
      "  Profiling timer resolution                      1ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            Yes\n",
      "    SPIR versions                                 1.2\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                \n",
      "  Device Extensions                               cl_khr_byte_addressable_store cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_3d_image_writes cl_khr_spir cl_khr_fp64 cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp64\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "Number of devices                                 1\n",
      "  Device Name                                     GeForce GTX 1050\n",
      "  Device Vendor                                   NVIDIA Corporation\n",
      "  Device Vendor ID                                0x10de\n",
      "  Device Version                                  OpenCL 1.2 CUDA\n",
      "  Driver Version                                  390.48\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 \n",
      "  Device Type                                     GPU\n",
      "  Device Topology (NV)                            PCI-E, 01:00.0\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               5\n",
      "  Max clock frequency                             1493MHz\n",
      "  Compute Capability (NV)                         6.1\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     1\n",
      "    Supported partition types                     None\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             1024x1024x64\n",
      "  Max work group size                             1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preferred work group size multiple              32\r\n",
      "  Warp size (NV)                                  32\r\n",
      "  Preferred / native vector sizes                 \r\n",
      "    char                                                 1 / 1       \r\n",
      "    short                                                1 / 1       \r\n",
      "    int                                                  1 / 1       \r\n",
      "    long                                                 1 / 1       \r\n",
      "    half                                                 0 / 0        (n/a)\r\n",
      "    float                                                1 / 1       \r\n",
      "    double                                               1 / 1        (cl_khr_fp64)\r\n",
      "  Half-precision Floating-point support           (n/a)\r\n",
      "  Single-precision Floating-point support         (core)\r\n",
      "    Denormals                                     Yes\r\n",
      "    Infinity and NANs                             Yes\r\n",
      "    Round to nearest                              Yes\r\n",
      "    Round to zero                                 Yes\r\n",
      "    Round to infinity                             Yes\r\n",
      "    IEEE754-2008 fused multiply-add               Yes\r\n",
      "    Support is emulated in software               No\r\n",
      "    Correctly-rounded divide and sqrt operations  Yes\r\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\r\n",
      "    Denormals                                     Yes\r\n",
      "    Infinity and NANs                             Yes\r\n",
      "    Round to nearest                              Yes\r\n",
      "    Round to zero                                 Yes\r\n",
      "    Round to infinity                             Yes\r\n",
      "    IEEE754-2008 fused multiply-add               Yes\r\n",
      "    Support is emulated in software               No\r\n",
      "  Address bits                                    64, Little-Endian\r\n",
      "  Global memory size                              4238737408 (3.948GiB)\r\n",
      "  Error Correction support                        No\r\n",
      "  Max memory allocation                           1059684352 (1011MiB)\r\n",
      "  Unified memory for Host and Device              No\r\n",
      "  Integrated memory (NV)                          No\r\n",
      "  Minimum alignment for any data type             128 bytes\r\n",
      "  Alignment of base address                       4096 bits (512 bytes)\r\n",
      "  Global Memory cache type                        Read/Write\r\n",
      "  Global Memory cache size                        81920 (80KiB)\r\n",
      "  Global Memory cache line size                   128 bytes\r\n",
      "  Image support                                   Yes\r\n",
      "    Max number of samplers per kernel             32\r\n",
      "    Max size for 1D images from buffer            134217728 pixels\r\n",
      "    Max 1D or 2D image array size                 2048 images\r\n",
      "    Max 2D image size                             16384x32768 pixels\r\n",
      "    Max 3D image size                             16384x16384x16384 pixels\r\n",
      "    Max number of read image args                 256\r\n",
      "    Max number of write image args                16\r\n",
      "  Local memory type                               Local\r\n",
      "  Local memory size                               49152 (48KiB)\r\n",
      "  Registers per block (NV)                        65536\r\n",
      "  Max number of constant args                     9\r\n",
      "  Max constant buffer size                        65536 (64KiB)\r\n",
      "  Max size of kernel argument                     4352 (4.25KiB)\r\n",
      "  Queue properties                                \r\n",
      "    Out-of-order execution                        Yes\r\n",
      "    Profiling                                     Yes\r\n",
      "  Prefer user sync for interop                    No\r\n",
      "  Profiling timer resolution                      1000ns\r\n",
      "  Execution capabilities                          \r\n",
      "    Run OpenCL kernels                            Yes\r\n",
      "    Run native kernels                            No\r\n",
      "    Kernel execution timeout (NV)                 Yes\r\n",
      "  Concurrent copy and kernel execution (NV)       Yes\r\n",
      "    Number of async copy engines                  2\r\n",
      "  printf() buffer size                            1048576 (1024KiB)\r\n",
      "  Built-in kernels                                \r\n",
      "  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_nv_create_buffer\r\n",
      "\r\n",
      "NULL platform behavior\r\n",
      "  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\r\n",
      "  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\r\n",
      "  clCreateContext(NULL, ...) [default]            No platform\r\n",
      "  clCreateContext(NULL, ...) [other]              Success [INTEL]\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No platform\r\n"
     ]
    }
   ],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workitems vs Workgroups\n",
    "Exploring different types of parallelism, using a more involved example, which is the Java string hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intel_context = pyopencl.Context(devices=intel_devices)\n",
    "intel_queue = pyopencl.CommandQueue(intel_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-07 10:00:28--  https://s3-eu-west-1.amazonaws.com/word-count-test-bucket/shakespeare_words.txt\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.20.12\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.20.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5458199 (5.2M) [text/plain]\n",
      "Saving to: ‘shakespeare_words.txt.1’\n",
      "\n",
      "shakespeare_words.t 100%[===================>]   5.21M   574KB/s    in 16s     \n",
      "\n",
      "2018-07-07 10:00:47 (327 KB/s) - ‘shakespeare_words.txt.1’ saved [5458199/5458199]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-eu-west-1.amazonaws.com/word-count-test-bucket/shakespeare_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_arrays_from_file(filename, N_words):\n",
    "    \"\"\"Reads a text file into numpy arrays.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    filename -- the file to read from\n",
    "    N_words -- maximum number of words to return\n",
    "    \n",
    "    Returns:\n",
    "    data -- numpy.Array ASCII with a single entry containing all of \n",
    "            the non-whitespace characters in the file.\n",
    "    offsets -- numpy.Array containing an integer element \n",
    "                for each word's offset\n",
    "    lengths -- numpy.Array containing an integer element \n",
    "                for each word's length  \n",
    "    \"\"\"\n",
    "    with open(filename,\"r\") as data_file:\n",
    "        # Reading data from file, and making it into one big string\n",
    "        data = [line[:-1] for line in data_file]\n",
    "        data_string = \"\".join(data)\n",
    "    \n",
    "        # Getting the lengths of each word\n",
    "        lengths = [len(word) for word in data]\n",
    "    \n",
    "        # Getting the start of each word\n",
    "        offsets = [0]\n",
    "        for word in data[:-1]:\n",
    "            offsets += [offsets[-1] + len(word)]\n",
    "        \n",
    "        # Testing that the offsets and counts are correct\n",
    "        for i,word in enumerate(data):\n",
    "            temp_word = data_string[offsets[i]:offsets[i]+lengths[i]] \n",
    "            if(word != temp_word): \n",
    "                print(\"Problem :\",word,\"!=\",temp_word)\n",
    "                raise(\"Data mismatch!\")\n",
    "            \n",
    "        # Converting data into numpy array\n",
    "        offsets_array = numpy.array(offsets[:N_words], dtype=numpy.int32)\n",
    "        lengths_array = numpy.array(lengths[:N_words], dtype=numpy.int32)\n",
    "        \n",
    "        end_last_word = offsets_array[-1] + lengths_array[-1]\n",
    "        byte_data_string = data_string[:end_last_word].encode(\"ascii\",\"ignore\")\n",
    "        data_array = numpy.array(byte_data_string)\n",
    "            \n",
    "    return data_array, offsets_array, lengths_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source data\n",
    "N_words = 2**20\n",
    "words, offsets, lengths = get_word_arrays_from_file(\"shakespeare_words.txt\", N_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_buffers(context, words, offsets, lengths):\n",
    "    \"\"\"Creates a read only PyOpenCL buffers for a numpy.Array\n",
    "    \n",
    "    Keyword arguments:\n",
    "    context -- pyopencl.Context that buffer will be created in.\n",
    "    words -- numpy.Array of words data\n",
    "    offsets -- numpy.Array of offsets data\n",
    "    lengths -- numpy.Array of lengths data\n",
    "    \n",
    "    Returns:\n",
    "    words_buffers -- pyopencl.Buffer in context for words array\n",
    "    offsets_buffers -- pyopencl.Buffer in context for offsets array\n",
    "    lengths_buffers -- pyopencl.Buffer in context for lengths array\n",
    "    \"\"\"\n",
    "    ro_mem_flags = pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.ALLOC_HOST_PTR\n",
    "    \n",
    "    # Source buffers\n",
    "    buffers = [pyopencl.Buffer(context,\n",
    "                               flags=ro_mem_flags,\n",
    "                               size=array.nbytes) \n",
    "               for array in (words,offsets,lengths)]\n",
    "    \n",
    "    wo_mem_flags = pyopencl.mem_flags.WRITE_ONLY\n",
    "    output_size = offsets.nbytes #int per word\n",
    "    \n",
    "    # Destination buffers\n",
    "    buffers += [pyopencl.Buffer(context,\n",
    "                                flags=wo_mem_flags,\n",
    "                                size=output_size)]\n",
    "    \n",
    "    return tuple(buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intel Buffers\n",
    "intel_buffers = create_data_buffers(intel_context, words, offsets, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "intel_program = build_program(\"java_hash.cl\", intel_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_words(queue, program, data_arrays, data_buffers, local_size=(32,)):\n",
    "    \"\"\"Copy data onto OpenCL device, does the hashing operation, copies result off.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    queue -- pyopencl.Queue \n",
    "    program -- compiled pyopencl.Program \n",
    "    data_arrays -- Source data arrays\n",
    "    data_buffers -- pyopencl.Buffers to use in pyopencl problem\n",
    "    hashes -- Empty numpy.Array for results\n",
    "    \n",
    "    Returns:\n",
    "    hashes -- Copy of numpy.Array of ints, containing the results\n",
    "    \"\"\"\n",
    "    words, offsets, lengths = data_arrays\n",
    "    words_buffer, offsets_buffer, lengths_buffer, hashes_buffer = data_buffers\n",
    "    \n",
    "    # Copying data onto the device\n",
    "                     # Words\n",
    "    copyon_events = [pyopencl.enqueue_copy(queue,\n",
    "                                           src=words,\n",
    "                                           dest=words_buffer),\n",
    "                     # Offsets\n",
    "                     pyopencl.enqueue_copy(queue,\n",
    "                                           src=offsets,\n",
    "                                           dest=offsets_buffer),\n",
    "                     # Lengths\n",
    "                     pyopencl.enqueue_copy(queue,\n",
    "                                           src=lengths,\n",
    "                                           dest=lengths_buffer)]\n",
    "    \n",
    "    # Hashing the words\n",
    "    kernel_event = program.java_hash_cached(queue,\n",
    "                                            (offsets.size,), #global size\n",
    "                                            local_size, #local size\n",
    "                                            words_buffer,offsets_buffer,lengths_buffer,hashes_buffer, #buffers\n",
    "                                            wait_for=copyon_events)\n",
    "    \n",
    "    \n",
    "    # Copying data off the device\n",
    "    hashes = numpy.empty(offsets.shape,dtype=numpy.int32)\n",
    "    copyoff_event = pyopencl.enqueue_copy(queue,\n",
    "                                          src=hashes_buffer,\n",
    "                                          dest=hashes,\n",
    "                                          wait_for=[kernel_event])\n",
    "    \n",
    "    #wait for copy-off to finish\n",
    "    copyoff_event.wait()\n",
    "    \n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "intel_hashes = hash_words(intel_queue, intel_program, \n",
    "                          (words, offsets, lengths,), \n",
    "                          intel_buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_java_hash(word):\n",
    "    \"\"\"Hashes a string into 32-bit integer value, as per the Java hash algorithm\"\"\"\n",
    "    h = 0\n",
    "    for c in word:\n",
    "        h *= 31\n",
    "        h += c\n",
    "        h &= 0xFFFFFFFF\n",
    "        \n",
    "    return ((h + 0x80000000) & 0xFFFFFFFF) - 0x80000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_words_raw = str(words)[2:-1]\n",
    "temp_words = [temp_words_raw[offset:(offset+length)].encode('ascii') for offset,length in zip(offsets[:N],lengths[:N])]\n",
    "ref_hashes = numpy.fromiter(map(ref_java_hash,temp_words),numpy.int32)\n",
    "\n",
    "for i,(intel_hash,ref_hash) in enumerate(zip(intel_hashes, ref_hashes)):\n",
    "    if(intel_hash != ref_hash): \n",
    "        print(i,\"Problem!\",intel_hash,ref_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48 ms ± 41.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hash_words(intel_queue, intel_program, (words, offsets, lengths,), intel_buffers, local_size=(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 ms ± 10 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy.fromiter(map(ref_java_hash,temp_words),numpy.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def ref_java_hash_numba(word):\n",
    "    \"\"\"Hashes a string into 32-bit integer value, as per the Java hash algorithm\"\"\"\n",
    "    h = 0\n",
    "    for c in word:\n",
    "        h *= 31\n",
    "        h += c\n",
    "        h &= 0xFFFFFFFF\n",
    "        \n",
    "    return ((h + 0x80000000) & 0xFFFFFFFF) - 0x80000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 ms ± 6.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy.fromiter(map(ref_java_hash_numba,temp_words),numpy.int32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
